\FloatBarrier
\section{Experimental Setup}

All benchmarks were executed on an 8 cores cpu at 3302 MHz base frequency with 32 GB RAM, running Windows 11. Source code was compiled using MINGW in Release mode with full optimisations enabled. The Google Benchmark framework ensured statistical stability through automatic iteration count adjustment and outlier rejection.

\textbf{Week 1.} Sorting experiments evaluated five algorithms across three input distributions: pre-sorted, reverse-sorted, and random binary sequences. Array sizes ranged from $10^4$ to $10^6$ elements. Graph traversal and shortest path benchmarks employed a small test graph comprising 5 vertices and 7 edges. For A* evaluation, a trivial heuristic $h(v) = 1$ was used to isolate algorithmic overhead from heuristic quality effects.

\textbf{Week 2.} Dynamic programming benchmarks compared brute-force and DP implementations for both subset-sum and 0/1 knapsack problems. For subset-sum, input sizes ranged from $n = 10$ to $n = 24$, with element values drawn uniformly from $[1, 100]$ and target set to approximately half the total sum. For knapsack, sizes ranged from $n = 10$ to $n = 25$ for brute-force comparisons. Knapsack capacity was fixed at $W = 1000$ throughout. Brute-force implementations were capped at $n \leq 30$ to prevent excessive runtimes.

\textbf{Week 3.} Network flow benchmarks evaluated the Edmonds-Karp maximum flow algorithm against the cycle-canceling minimum-cost maximum flow algorithm. Test networks were randomly generated with a wide layered topology comprising four layers (source, two intermediate layers, sink), ensuring that source and sink degrees scaled proportionally with network size. Vertex counts ranged from $V = 10$ to $V = 2000$, with edge density fixed at $E = 3V$ to maintain sparse connectivity. Edge capacities were drawn uniformly from $[1, 100]$ and costs from $[1, 20]$.
