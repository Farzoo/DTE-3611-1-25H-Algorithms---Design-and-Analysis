\FloatBarrier
\section{Discussion}

\subsection{Classical Algorithms}

The experimental results presented in this study validate theoretical complexity predictions across all algorithm families examined. Linear-time sorting algorithms (counting sort, radix sort) demonstrate clear advantages over comparison-based alternatives when applicable to bounded-range integer inputs. Binary sort, despite achieving asymptotically optimal $O(n \log n)$ complexity, suffers from substantial constant-factor overhead attributable to dynamic memory allocation and suboptimal cache utilisation patterns inherent to pointer-based tree structures.

The unified Dijkstra/A* implementation demonstrates that careful abstraction design enables code reuse without performance degradation. By parameterising the priority function, both algorithms share identical core logic while preserving their distinct characteristics.

\subsection{NP-Complete Problems}

The knapsack benchmarks provide compelling empirical evidence for the theoretical distinction between exponential and pseudo-polynomial complexity. The observed speedup factor of $9 \times 10^5$ at $n = 25$ corresponds closely to the theoretical ratio $2^{25} / (25 \times 1000) \approx 1.34 \times 10^6$, confirming that our implementations faithfully exhibit their expected asymptotic behaviours.

For subset-sum, the more modest speedup reflects implementation choices rather than algorithmic deficiency. When seeking a single Boolean answer, DP would dominate unambiguously. Our decision to enumerate all solutions transforms both algorithms into output-sensitive procedures whose runtime depends on solution count.

The NP-completeness of both problems, established through the reduction chain $\text{3-SAT} \leq_P \text{Subset-Sum} \leq_P \text{Knapsack}$, implies that no polynomial-time algorithm exists under standard complexity assumptions. Dynamic programming circumvents this barrier by exploiting the numeric structure of specific instances: when capacity $W$ remains polynomially bounded in $n$, the $O(n \cdot W)$ algorithm becomes practically efficient despite theoretical intractability.

\subsection{Network Flow Algorithms}

The network flow experiments reveal favourable empirical behaviour for the Edmonds-Karp algorithm on sparse layered networks. With scaling closer to $O(V^2)$ than the theoretical $O(VE^2)$ bound, Edmonds-Karp proves highly efficient for maximum flow computation when network structure produces short augmenting paths.

The cycle-canceling algorithm, which extends maximum flow to minimum-cost maximum flow, incurs substantial additional overhead. Since cycle-canceling first computes a maximum flow and then iteratively eliminates negative-cost cycles, the measured ratio (up to $213\times$ at $V = 2000$) reflects the cost of this optimisation phase rather than a direct algorithmic comparison. This distinction is practically significant: applications requiring only maximum flow values should employ Edmonds-Karp alone, reserving cycle-canceling for scenarios where transportation cost minimisation is essential.

\subsection{Amortised Analysis and Memory Allocation}

The amortised analysis benchmarks validate the theoretical $O(1)$ amortised cost of \texttt{std::vector::push\_back}. Despite individual reallocations costing $O(n)$, the doubling strategy ensures that total insertion cost remains linear, yielding constant amortised overhead. The modest 1.5$\times$ slowdown without pre-allocation confirms that reallocation costs distribute efficiently across insertions.

Dictionary comparisons reveal the performance trade-offs between ordered and unordered containers. While \texttt{std::map} guarantees $O(\log n)$ worst-case operations, \texttt{std::unordered\_map} achieves 13--15$\times$ speedup for lookups at larger sizes, consistent with $O(1)$ amortised hash table access. The choice between containers thus depends on whether ordering is required or whether worst-case guarantees take precedence over average-case performance.

The polymorphic memory resource benchmarks demonstrate that allocation strategy significantly impacts performance for allocation-intensive workloads. The monotonic buffer resource achieves speedups ranging from 1.7$\times$ to 5.9$\times$ over standard allocation for workloads involving many small allocations, owing to its bump-pointer strategy and no-op deallocation. The pool resource exhibits workload-dependent behaviour: it excels for fixed-size allocations (achieving up to 4.7$\times$ speedup for map node allocations) but shows negligible or negative improvement for variable-size allocations such as strings. These findings suggest that PMR adoption should be guided by allocation pattern characteristics: monotonic resources for batch-style processing, pools for fixed-size node allocations with high reuse.

\subsection{Limitations}

Several constraints affect this study. Graph traversal and shortest path benchmarks (Week 1) employed minimal test instances (5 vertices), precluding meaningful validation of asymptotic complexity claims for these algorithms. However, network flow experiments (Week 3) utilised substantially larger sparse networks with up to 2000 vertices, enabling more meaningful empirical complexity observations. String matching measurements fell below instrumentation resolution thresholds. For the NP-complete problems, our subset-sum implementation's choice to enumerate all solutions complicates direct comparison with decision-only algorithms. Furthermore, knapsack benchmarks used fixed capacity $W = 1000$; varying $W$ would reveal the pseudo-polynomial nature more transparently.

\subsection{Future Work}

Regarding network flow, our cycle-canceling implementation follows Klein's basic algorithm; incorporating the Goldberg-Tarjan refinement would guarantee polynomial-time termination for integer flows. Additionally, extending the PMR benchmarks to include real-world allocation patterns from production applications would provide more actionable guidance for practitioners.
